{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RPenj5Yxj0mO"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCp4KNhvkakN"
   },
   "source": [
    "### Read this First\n",
    "\n",
    "#### Remember that `tab` is is useful for autocompletion.\n",
    "\n",
    "#### Remember that `shift + tab` is useful for rapidly obtaining usage + documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsEv_O527qu8"
   },
   "source": [
    "###**Transformations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Y29w4PT8JgT"
   },
   "source": [
    "**In class we covered some of the transformation types that we could apply to images, and we'll go over some of them here using OpenCV's function to see firsthand what they do to images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "7vfZC53h-YDP",
    "outputId": "b1f005bf-7b33-45b0-e7c9-251ff5f44ddf"
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-k8sx3e60\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ec3171e46625>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'image_transformation.jpg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMREAD_COLOR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-k8sx3e60\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread('image_transformation.jpg', cv2.IMREAD_COLOR)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "rows, cols, channels = image.shape\n",
    "plt.figure()\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8l879qA-DFp"
   },
   "source": [
    "**Recall from class that we can represent transformations using matrices. As we are operating in 2D but with homogeneous coordinates, every linear transformation can be represented as a 3x3 matrix.**\n",
    "\n",
    "**Below, create a 3x3 matrix that represents a scaling of 0.5 in the x-dimension. Then, using OpenCV's warpprojective, apply this to the loaded image and plot it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFqYvlMb8I5D"
   },
   "outputs": [],
   "source": [
    "\n",
    "transform = np.array([ [0.5 , 0 , 0 ], [0,1,0], [0,0,1]])\n",
    "output = cv2.warpPerspective(image, transform, (cols, rows))\n",
    "plt.figure()\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BianXUHoADa_"
   },
   "source": [
    "**We see that there is black space to the right of the image (assuming you did it correctly). This is because our image has changed size. This type, change the (cols, rows) input parameter of warpPerspective so that the output image does not contain that black space to the right. Display the result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nm776J7lAlY3"
   },
   "outputs": [],
   "source": [
    "transform = np.array([ [0.5 , 0 , 0 ], [0,1,0], [0,0,1]])\n",
    "output = cv2.warpPerspective(image, transform, (cols//2, rows))\n",
    "plt.figure()\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-FrSypRAriu"
   },
   "source": [
    "**Now let's see what happens with a translation. Create a 3x3 matrix that represents a translation by 60 pixels to the right in the x direction and 120 pixels up in the y direction. Use warpPerspective (with output image size the same as original image size) to apply this transformation and display the resulting image.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_XcXGr8A-2R"
   },
   "outputs": [],
   "source": [
    "transform = np.array([ [1.0 , 0 , 60 ], [0,1,120], [0,0,1]])\n",
    "output = cv2.warpPerspective(image, transform, (cols, rows))\n",
    "plt.figure()\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9zUCfXRB_Cj"
   },
   "source": [
    "**Now, create a 3x3 matrix that represents a rotation by 30 degrees clockwise, and apply this to the image via warpPerspective, with output image size the same as original image size. Display the result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQhiVUjwAjQL"
   },
   "outputs": [],
   "source": [
    "angel =  np.pi/180.0 * 30.0\n",
    "transform = np.array([ [np.cos(angel) , -np.sin(angel) ,0 ], [np.sin(angel),np.cos(angel),0], [0,0,1]])\n",
    "output = cv2.warpPerspective(image, transform, (cols, rows))\n",
    "plt.figure()\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtL6ExA5CmCU"
   },
   "source": [
    "**Where is the center of rotation in this case? How would you change it so that the image rotates about its center?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMCkyIxWCsZZ"
   },
   "source": [
    "the center is (0,0) upper left corner of the picture, to make the image rotates about its center, I would translate the center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEKlUV6UCtH0"
   },
   "source": [
    "**Create a transformation matrix that performs a rotation of 30 degrees clockwise with the center of rotation about the center of the image. You can do this by computing where the new center of the image would be after the rotation, and adding a translation to move it back to the center. Again, apply this to our image via warpPerspective with output image size the same as original image size, and display the result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31aamAp-DAER"
   },
   "outputs": [],
   "source": [
    "angel =  np.pi/180.0 * 30.0\n",
    "center = np.array([cols//2 ,rows//2,0])\n",
    "transform = np.array([ [np.cos(angel) , -np.sin(angel) ,0 ], [np.sin(angel),np.cos(angel),0], [0,0,1]])\n",
    "new_center = np.dot(transform , np.transpose( center ) )\n",
    "translation = new_center - center\n",
    "transform[: , 2] = transform[: , 2] - np.transpose(translation) \n",
    "output = cv2.warpPerspective(image, transform, (cols, rows))\n",
    "plt.figure()\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wESt7TpuFiqg"
   },
   "source": [
    "**For homogeneous coordinates, is the rotation performed first or the translation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VIAiFQ_FzYT"
   },
   "source": [
    "the rotation performs first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JjmY7JRGm6u"
   },
   "source": [
    "**Now let's see what shear does to the image. Set the x component of the shear parameter to 2, and apply that to the image, and display the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHtTHEGrFmc6"
   },
   "outputs": [],
   "source": [
    "angel =  0\n",
    "sh =2.0\n",
    "transform = np.array([ [1, sh ,0 ], [0,1,0], [0,0,1]])\n",
    "output = cv2.warpPerspective(image, transform, (cols, rows))\n",
    "plt.figure()\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angel =  0\n",
    "sh =0.1\n",
    "transform = np.array([ [1, sh ,0 ], [0,1,0], [0,0,1]])\n",
    "output = cv2.warpPerspective(image, transform, (cols, rows))\n",
    "plt.figure()\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZe9z7pnHAtb"
   },
   "source": [
    "**What does this do to the image? Play around with the shear parameters so you have a better understanding.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsWyAHrZHE_E"
   },
   "source": [
    "It is stretching the image along lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AusyhnC2HGbM"
   },
   "source": [
    "**Now create a 3x3 transformation matrix that represents a 15 degree counterclockwise rotation, a scaling by 1.5 in the x direction, a shear in the y direction of 1.2, and finally a translation by (-300, -200). Display the result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebk3Y_rrH66_"
   },
   "outputs": [],
   "source": [
    "\n",
    "angel = -15.0 * np.pi/180.0\n",
    "transform = np.array([ [np.cos(angel) , -np.sin(angel) ,0 ], [ np.sin(angel), np.cos(angel) , 0], [ 0 , 0 , 1 ]])\n",
    "transform = np.dot(np.array([ [1.5  , 0 ,0 ], [0 , 1 , 0], [ 0 , 0 , 1 ]]) , transform)\n",
    "transform = np.dot(np.array([ [1  , 0 ,0 ], [1.2 , 1 , 0], [ 0 , 0 , 1 ]]), transform)\n",
    "transform = np.dot( np.array([ [1  , 0 ,-300 ], [0,1 , -200], [ 0 , 0 , 1 ]]) , transform )\n",
    "output = cv2.warpPerspective(image, transform, (cols*2, rows*2))\n",
    "plt.figure()\n",
    "plt.imshow(output)\n",
    "print(transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5JH3VcBI_rF"
   },
   "source": [
    "**How can we tell that this is an affine transformation? What is preserved for affine transformations?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVAsJ-CAJGFI"
   },
   "source": [
    "the last row is [0,0,1], the origin remains the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlPOzYx5JGnH"
   },
   "source": [
    "**How do we tell from a transformation matrix that it is an affine transformation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZ1U-Ip9JKX_"
   },
   "source": [
    "the last row is [0,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISLxDZboJLKi"
   },
   "source": [
    "**Now set the last row of the previous transformation matrix to be $[-0.002, 0.004, 1]$, and apply the transformation to the image and display the result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5jIARfPI_Hl"
   },
   "outputs": [],
   "source": [
    "transform[2,:] = np.array([-0.002, 0.004, 1])\n",
    "print(transform)\n",
    "output = cv2.warpPerspective(image, transform, (cols, rows))\n",
    "plt.figure()\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Up6PlISNkv-"
   },
   "source": [
    "**How can you tell from the image that this is no longer an affine transformation and is instead a homography?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d9K3YnZNqoG"
   },
   "source": [
    "the left and right edges of the original image are no longer paralled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_es0bcaOJGw"
   },
   "source": [
    "**Even though there are $3\\text{ rows }\\cdot 3\\text{ cols }=9$ parameters for a homography, there are only 8 degrees of freedom. This is because homographies are only defined up to scale, as when we convert from homogeneous coordinates back to 2D coordinates, we divide by the last component, so the scale of the last component is constrained to be 1.**\n",
    "\n",
    "**Demonstrate this by multiplying the transformation obtained above by 3, and using that to transfrom the image, and display the result. You should observe that the image is identical to that above.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZF6YK_4eOEDz"
   },
   "outputs": [],
   "source": [
    "angel = -15.0 * np.pi/180.0\n",
    "transform = np.array([ [np.cos(angel) , -np.sin(angel) ,0 ], [ np.sin(angel), np.cos(angel) , 0], [ 0 , 0 , 1 ]])\n",
    "transform = np.dot(np.array([ [1.5  , 0 ,0 ], [0 , 1 , 0], [ 0 , 0 , 1 ]]) , transform)\n",
    "transform = np.dot(np.array([ [1  , 0 ,0 ], [1.2 , 1 , 0], [ 0 , 0 , 1 ]]), transform)\n",
    "\n",
    "transform = np.dot( np.array([ [1  , 0 ,-300 ], [0,1 , -200], [ 0 , 0 , 1 ]]) , transform )\n",
    "\n",
    "transform = transform*3\n",
    "output = cv2.warpPerspective(image, transform, (cols, rows))\n",
    "plt.figure()\n",
    "plt.imshow(output)\n",
    "print(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fq7qdePZPfJ0"
   },
   "source": [
    "###**Image Warping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EePbKqmPhIj"
   },
   "source": [
    "**In above, we've seen how given a transformation matrix we can use OpenCV to transform our input image accordingly. Here, we'll see how we can perform these transformations ourselves.**\n",
    "\n",
    "**To do this, let's first create a transformation matrix that represents a 30 degree clockwise rotation about the center of the image (you've already done this earlier. Use OpenCV to perform the transformation and display the image. This is what we want to achieve.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39NEATkzVuqU"
   },
   "outputs": [],
   "source": [
    "angel = 30.0 * np.pi/180.0\n",
    "transform = np.array([ [np.cos(angel) , -np.sin(angel) ,0 ], [ np.sin(angel), np.cos(angel) , 0], [ 0 , 0 , 1 ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Hpba50pWJ81"
   },
   "source": [
    "**We can first attempt to do image warping by looping through every single pixel of the image, performing the transformation on the coordinates of that pixel, and assign the resulting coordinate to have the same pixel intensities as the original. This is forward warping.**\n",
    "\n",
    "**We can do this initially by rounding the resulting coordinates to the closest integers, and we ignore all pixels that map to locations outside of the output image matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQOY3rY9Wx3Y"
   },
   "outputs": [],
   "source": [
    "output_image = np.zeros((rows, cols, 3), dtype=int)\n",
    "\n",
    "for i in range(rows):\n",
    "  for j in range(cols):\n",
    "    \n",
    "    # Get the resulting pixel coordinate after applying the transformation above for pixel (j, i)\n",
    "    # Round the pixel coordinates to the nearest integer values\n",
    "    coord = np.array([j,i,1],dtype = np.float)\n",
    "    new_j = int(np.round( np.sum(transform[0,:] * coord )))\n",
    "    new_i = int(np.round( np.sum(transform[1,:] * coord )))\n",
    "\n",
    "    # Check that the coordinates are within bounds\n",
    "    if( 0 <= new_j and new_j < cols and 0<=new_i and new_i<rows):\n",
    "    # If they are, then assign that coordinate in the output_image to have the same pixel values\n",
    "    # as the input pixel from the original image\n",
    "        output_image[new_i,new_j,:] = image[i,j,:]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0asyjoxZnI3"
   },
   "source": [
    "**The image looks roughly like the one transformed via OpenCV. But what is strange about the image? What do you think could be the issue?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixjvlgVrZvUA"
   },
   "source": [
    "there are cross-like patterns in the picture, I think its the 'round' operation caused the issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wr5-KD9FZvxx"
   },
   "source": [
    "**In order to combat this problem, we need to interpolate the pixel contributions instead of rounding it to the nearest pixel value. That is, if we have a non-integer pixel coordinate, we add contribution to the overlapped pixels proportional to the area of overlap.**\n",
    "\n",
    "**To do that, let's first create a helper function that given an x and y, finds the list of x and y pixel coordinates that it overlaps with, along with area of overlap with that pixel.**\n",
    "\n",
    "**For example, given $(x, y) = (10.3, 20.2)$, the function should return $$[[10, 20, 0.56], [10, 21, 0.14], [11, 20, 0.24], [11, 21, 0.06]]$$ since $(10.3, 20.2)$ overlaps with $(11, 21)$ by $0.3$ in the x-direction and $0.2$ in the y-direction, which results in area $0.3 \\cdot 0.2 = 0.06$, as seen by the last term in the list.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvK2PsFFai83"
   },
   "outputs": [],
   "source": [
    "def get_pixel_contributions(x, y):\n",
    "    \"\"\" Gets the overlapped pixels along with the relative contributions.\n",
    "          Args:\n",
    "            x: The x-coordinate\n",
    "            y: The y-coordinate\n",
    "          Return:\n",
    "            pixel_contributions: List of [x, y, contribution] where (x, y) are the coordinates of\n",
    "                                 pixels that the input overlaps with.\n",
    "    \"\"\"\n",
    "    x_list = [np.floor(x) , np.floor(x)+1]\n",
    "    y_list = [np.floor(y) , np.floor(y)+1]\n",
    "    pixel_contributions = []\n",
    "        \n",
    "    for tmp_x in x_list:\n",
    "        for tmp_y in y_list:\n",
    "            overlapx = 1.0 - np.abs(x-tmp_x)\n",
    "            overlapy = 1.0 - np.abs(y-tmp_y)\n",
    "            pixel_contributions.append([int(tmp_x), int(tmp_y), overlapx*overlapy ])\n",
    "        \n",
    "    return pixel_contributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stqofs3FipHU"
   },
   "source": [
    "**Now we have that, we can see how interpolation does. Run the code below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9eXrtKFPZ82R"
   },
   "outputs": [],
   "source": [
    "def forward_warping(transform, image):\n",
    "\n",
    "  output_image = np.zeros((rows, cols, 3), dtype=int)\n",
    "\n",
    "  for i in range(rows):\n",
    "    for j in range(cols):\n",
    "\n",
    "      # Get the resulting pixel coordinate after applying the transformation above for pixel (j, i()\n",
    "      result = transform @ np.array([[j], [i], [1]])\n",
    "\n",
    "      # Get the interpolated pixel coordinates and the corresponding contributions\n",
    "      interpolations = get_pixel_contributions(result[0, 0], result[1, 0])\n",
    "\n",
    "      for k in range(4):\n",
    "        x = interpolations[k][0]\n",
    "        y = interpolations[k][1]\n",
    "        scale = interpolations[k][2]\n",
    "        if -1 < x < cols and -1 < y < rows:\n",
    "          output_image[y, x, 0] += int(scale * image[i, j, 0])\n",
    "          output_image[y, x, 1] += int(scale * image[i, j, 1])\n",
    "          output_image[y, x, 2] += int(scale * image[i, j, 2])\n",
    "  \n",
    "  return output_image\n",
    "\n",
    "output_image = forward_warping(transform, image)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ST31K2O3i5bs"
   },
   "source": [
    "**This should look much better.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LY_P8N7Mi-xx"
   },
   "source": [
    "**Though this looks nice on our image, there are still some artifacts in the image. This will be amplified when we have shear. Using the matrix below, plot the transformed image with that matrix first via OpenCV's warpPerspective, then using our implementation forward_warping above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nA7dP4vXXcTe"
   },
   "outputs": [],
   "source": [
    "transform = np.array([[ 1.75947159e+00,  2.58819045e-01, -3.00000000e+02],\n",
    " [ 9.00291946e-01,  9.65925826e-01, -2.00000000e+02],\n",
    " [ 0.00000000e+00, 0.00000000e+00,  1.00000000e+00]])\n",
    "output = cv2.warpPerspective(image, transform, (cols, rows))\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(output)\n",
    "output_image = forward_warping(transform, image)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQnT62kEovTX"
   },
   "source": [
    "**You'll probably be able to se the line artifacts across the entire image.**\n",
    "\n",
    "**Hence an alternative to this is inverse warping, where instead of transforming each pixel from the input image, we take every pixel of the output image, perform the inverse transform, and see where it maps to from the input image and take those values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNH45MQrpY6M"
   },
   "outputs": [],
   "source": [
    "def inverse_warping(transform, image):\n",
    "\n",
    "    output_image = np.zeros((rows, cols, 3), dtype=int)\n",
    "    transform = np.linalg.inv(transform)\n",
    "  # Invert the transformation matrix\n",
    "    \n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "\n",
    "          # Get the resulting pixel coordinate after applying the inverse transformation above for pixel (j, i()\n",
    "            result = transform @ np.array([[j], [i], [1]])\n",
    "\n",
    "          # Use interpolation \n",
    "            interpolations = get_pixel_contributions(result[0, 0], result[1, 0])\n",
    "\n",
    "          # Do something similar to forward_warping but with inverse warping\n",
    "            for k in range(4):\n",
    "                    x = interpolations[k][0]\n",
    "                    y = interpolations[k][1]\n",
    "                    scale = interpolations[k][2]\n",
    "                    if -1 < x < cols and -1 < y < rows:\n",
    "                        output_image[i, j, 0] += int(scale * image[y, x, 0])\n",
    "                        output_image[i, j, 1] += int(scale * image[y, x, 1])\n",
    "                        output_image[i, j, 2] += int(scale * image[y, x, 2])\n",
    "\n",
    "    return output_image\n",
    "\n",
    "output_image = inverse_warping(transform, image)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EErgQ_ywqnd-"
   },
   "source": [
    "**You should notice that the line artifacts are gone and the result pretty much looks like OpenCV's implementation of warpPerspective (although it's quite a lot slower).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkLfUFiVqvMr"
   },
   "source": [
    "###**Computing Transformations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xxzJy-xGuO1"
   },
   "source": [
    "**Now that we know how to perform transformations, given two images, how do we find the transformation between them?**\n",
    "\n",
    "**We've been talking about features for the past couple of weeks, and this is where they come in - features are needed to find matching locations in different images so that we can estimate a transformation that brings one image to another.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBeZ6yjpJivy"
   },
   "source": [
    "**We'll first explore how we can find a translation transformation between two images using features.**\n",
    "\n",
    "**Defined below are the image_features, which are some features of our image. On display_image, draw red circles around the image_features, and display them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bI-ScoavrzTO"
   },
   "outputs": [],
   "source": [
    "image_features = [[326, 60], [285, 249], [366, 249], [40, 412]]\n",
    "display_image = image.copy()\n",
    "for feature in image_features:\n",
    "    cv2.circle(display_image, (feature[0],feature[1]), 3, (255,0,0),-1)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(display_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJDfmAQjJzK-"
   },
   "source": [
    "**Say we have another image that is our original image but with a translation. Shown below are the same features, but in the translated image. Draw circles around these translated_features in blue on the same display image drawn above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rv33ajKTJ-rW"
   },
   "outputs": [],
   "source": [
    "translated_features = [[424, 45], [385, 233], [470, 230], [140, 399]]\n",
    "for feature in translated_features:\n",
    "    cv2.circle(display_image, (feature[0],feature[1]), 3, (0,0,255),-1)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(display_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3PnGrwrKZAN"
   },
   "source": [
    "**Now we need to find the transformation/translation that brings the red points to the blue points. We saw in lecture that we can do this using least squares, by setting up an $At=b$ relationship.**\n",
    "\n",
    "**Below, fill the $A$ matrix and $b$ vector with the correct values, and compute $t=(A^TA)^{-1}A^Tb$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17ocfvzjKy2j"
   },
   "outputs": [],
   "source": [
    "A = np.zeros((8, 2))\n",
    "b = np.zeros((8, 1))\n",
    "for i in range(4):\n",
    "    A[i*2 , 0] = 1.0\n",
    "    A[i*2+1,1] = 1.0\n",
    "    b[i*2,0] = translated_features[i][0] - image_features[i][0]\n",
    "    b[i*2+1,0] = translated_features[i][1] - image_features[i][1]\n",
    "t =  np.linalg.inv(np.transpose(A) @ A) @ np.transpose(A) @ b\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6R7Ei3l5K2Ql"
   },
   "source": [
    "**Reshape $t$ to be a transformation matrix, and apply the transformation to our original image. Draw red circles on the transformed image around translated_features. Display the result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYwu0XUJzYQE"
   },
   "outputs": [],
   "source": [
    "transform = np.identity(3)\n",
    "transform[0 , 2 ] = t[0]\n",
    "transform[1 , 2 ] = t[1]\n",
    "\n",
    "rows, cols, channels = image.shape\n",
    "output = cv2.warpPerspective(image, transform, (cols, rows))\n",
    "\n",
    "display_image = output.copy()\n",
    "for feature in translated_features:\n",
    "    cv2.circle(display_image, (feature[0],feature[1]), 3, (255,0,0),-1)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(display_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NzHoTlu1sh7"
   },
   "source": [
    "**How many points do we need to find the translation? Why do we use more than needed here?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pt50gCDw1yjE"
   },
   "source": [
    "at least 1 pair of points( 2 unknowns, 2 equations per match, need at least 1 match), we use more than we needed for better peformance(less sensitive to noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxNP8KyvLW2v"
   },
   "source": [
    "**Now let's try to find an affine transformation. Shown below are the coordinates of the same image features after the image has been transformed by an affine transformation. Draw red circles around the image_features on display_image, and blue circles around the affine_features on display_image, and display the result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvqqBPEXMYnH"
   },
   "outputs": [],
   "source": [
    "affine_features = [[289.117, 151.45], [265.9, 297.1], [408.4, 370.0], [-122.99, 233.97]]\n",
    "display_image = image.copy()\n",
    "for feature in image_features:\n",
    "    cv2.circle(display_image, (feature[0],feature[1]), 3, (255,0,0),-1)\n",
    "for feature in affine_features:\n",
    "    cv2.circle(display_image, (int(feature[0]),int(feature[1])), 3, (0,0,255),-1)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(display_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAZcYstVMwUL"
   },
   "source": [
    "**To find the affine transformation, we also can set up a least squares formulation in the form $At=b$ again, but defined slightly differently. Like in the translation case, fill the $A$ matrix and $b$ vector appropriately, and solve for $t$, using np.linalg.lstsq.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMBRxrua1gmB"
   },
   "outputs": [],
   "source": [
    "A = np.zeros((8, 6))\n",
    "b = np.zeros((8, 1))\n",
    "for i in range(4):\n",
    "    A[i*2 , 0:3] = [image_features[i][0], image_features[i][1], 1.0]\n",
    "    A[i*2+1 , 3:6] = [image_features[i][0], image_features[i][1], 1.0]\n",
    "    b[i*2,0] = affine_features[i][0]\n",
    "    b[i*2+1,0] = affine_features[i][1]\n",
    "t = np.linalg.lstsq(A,b,rcond=-1)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_FMQJHRNo-Q"
   },
   "source": [
    "**Convert our solution $t$ to a transformation matrix, and apply that transformation to our image using OpenCV, and draw the corresponding affine_features as red circles in our transformed image. Display the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scyhJvCFNLJ1"
   },
   "outputs": [],
   "source": [
    "transform = np.identity(3)\n",
    "ret = np.array(t[0])\n",
    "print(ret.shape)\n",
    "transform[0 , 0:3 ] = [ret[0],ret[1],ret[2] ]\n",
    "transform[1 , 0:3 ] = [ret[3],ret[4],ret[5] ]\n",
    "transform[2 , 0:3 ] = [0 , 0 ,1]\n",
    "output = cv2.warpPerspective(image, transform, (cols, rows))\n",
    "rows, cols, channels = image.shape\n",
    "\n",
    "display_image = output.copy()\n",
    "for feature in affine_features:\n",
    "    cv2.circle(display_image, (int(feature[0]),int(feature[1])), 3, (255,0,0),-1)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(display_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4av4KvBNyJq"
   },
   "source": [
    "**We can also do this homographies, but we're not going to do this in our notebook (this is long enough I think)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0ZV3SsF7BT9"
   },
   "source": [
    "###**RANSAC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etfJHsk3EuiL"
   },
   "source": [
    "**We see in the computing transformation section above, solving using least squares formulation works well, but this only works when all of our correspondences are good matches - if there are outliers, then the transform computed will be off.**\n",
    "\n",
    "**Hence to combat this, we'll use RANSAC. Load the image below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7y0S673W_lRq"
   },
   "outputs": [],
   "source": [
    "img1 = cv2.imread('ransac_image.jpg', cv2.IMREAD_COLOR)\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "rows, cols, channels = img1.shape\n",
    "plt.figure()\n",
    "plt.imshow(img1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6wvRq-LFBVE"
   },
   "source": [
    "**Using the transform below, we can warp the image to it's rotated and shifted. Note that here we have the transformation, but in real situations we will have two different images and the goal will be to reconstruct the transformation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxssJcw7_6KK"
   },
   "outputs": [],
   "source": [
    "transform = np.array([[0.8660254, -0.5, 84.11542732], \n",
    "                      [0.5, 0.8660254, -73.92304845],\n",
    "                      [0.0, 0.0, 1.0]])\n",
    "\n",
    "img2 = cv2.warpPerspective(img1, transform, (cols, rows))\n",
    "plt.figure()\n",
    "plt.imshow(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5yOZdE9Ffmm"
   },
   "source": [
    "**Now that we have img1 and img2, we want to reconstruct the transformation. To do this, we need to find features that correspond between both images. Though we've learnt how to do this ourselves, let's just use OpenCV functions below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvr3WNfP_zEh"
   },
   "outputs": [],
   "source": [
    "# Use Oriented Fast and Rotated Brief feature detector\n",
    "orb = cv2.ORB_create()\n",
    "kp1, des1 = orb.detectAndCompute(img1,None)\n",
    "kp2, des2 = orb.detectAndCompute(img2,None)\n",
    "\n",
    "# Create a BFMatcher object\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Match descriptors\n",
    "matches = bf.match(des1,des2)\n",
    "\n",
    "# Sort them in the order of their distance.\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "# Draw first 10 matches\n",
    "img3 = cv2.drawMatches(img1, kp1, img2, kp2, matches[:10], None, flags=2)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(img3), plt.show()\n",
    "\n",
    "# Draw last few matches\n",
    "img4 = cv2.drawMatches(img1, kp1, img2, kp2, matches[280:], None, flags=2)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(img4), plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgWeIhocGWXE"
   },
   "source": [
    "**We see that even though there are good matches (shown in the first 10 matches), there are also bad matches (shown in the last few matches). If we use least squares, the bad matches will skew our results. Hence we should use RANSAC to eliminate contribution from outliers.**\n",
    "\n",
    "**Below is a short function that will help us get the coordinates of the matching keypoints, since the kp1 and kp2 are lists of keypoint objects.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NyrG-AUFZ91"
   },
   "outputs": [],
   "source": [
    "def get_keypoint_coord_from_match(matches, kp1, kp2, index):\n",
    "  \"\"\" Gets the keypoint coordinates that correspond to matches[index].\n",
    "      For example, if we want to get the coordinates of the keypoints corresponding\n",
    "      to the 10th matching pair, we would be passing\n",
    "      \n",
    "              get_keypoint_coord_from_match(matches, kp1, kp2, 10)\n",
    "      \n",
    "      Then it will return keypoint1, keypoint2, where\n",
    "      keypoint1: [x, y] coordinate of the keypoint in img1 that corresponds to matches[10]\n",
    "      keypoint2: [x, y] coordinate of the keypoint in img2 that corresponds to matches[10]\n",
    "  \"\"\"\n",
    "\n",
    "  keypoint1 = [kp1[matches[index].queryIdx].pt[0], kp1[matches[index].queryIdx].pt[1]]\n",
    "  keypoint2 = [kp2[matches[index].trainIdx].pt[0], kp2[matches[index].trainIdx].pt[1]]\n",
    "  return keypoint1, keypoint2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oV8U3HRkJt82"
   },
   "source": [
    "**Implement RANSAC below by completing the script below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcTmfzfPA_IS"
   },
   "outputs": [],
   "source": [
    "# Some parameters\n",
    "num_samples = 3\n",
    "num_trials = 3000\n",
    "total_matches = len(matches)\n",
    "inlier_thresh = 5\n",
    "\n",
    "# To keep track of the best transformation\n",
    "xform = np.zeros((3, 3))\n",
    "most_inliers = 0\n",
    "\n",
    "# Loop through num_trials times\n",
    "for i in range(num_trials):\n",
    "\n",
    "      # Randomly choose num_samples indices from total number of matches\n",
    "    choices = np.random.choice(total_matches, num_samples, replace=False)\n",
    "\n",
    "      # Get the matching keypoint coordinates from those indices\n",
    "    keypoints1 = []\n",
    "    keypoints2 = []\n",
    "    for choice in choices:\n",
    "        keypoint1, keypoint2 = get_keypoint_coord_from_match(matches, kp1, kp2, choice)\n",
    "        keypoints1.append(keypoint1)\n",
    "        keypoints2.append(keypoint2) \n",
    "      # Use getAffineTransform to get our transformation matrix\n",
    "    keypoints1 = np.float32(keypoints1)\n",
    "    keypoints2 = np.float32(keypoints2)\n",
    "    M = cv2.getAffineTransform(keypoints1, keypoints2)\n",
    "    transformation = np.zeros((3,3),dtype = np.float)\n",
    "    transformation[0,:] = M[0,:]\n",
    "    transformation[1,:] = M[1,:]\n",
    "    transformation[2,:] = [0,0,1.0]\n",
    "    num_inliers = 0\n",
    "\n",
    "      # Loop through all of our matches\n",
    "    for j in range(total_matches):\n",
    "        \n",
    "        # Get the coordinates of the matching features using get_keypoint_coord_from_match\n",
    "        point1, point2 = get_keypoint_coord_from_match(matches, kp1, kp2, j)\n",
    "\n",
    "        # Compute the expected coordinate after transforming the keypoint from img1\n",
    "        # using the computed affine transformation\n",
    "        p1 = np.array([point1[0],point1[1],1.0])\n",
    "        exp_coord =  transformation @ p1\n",
    "\n",
    "        # If the distance between the expected coordinate and the actual coordinate in im2\n",
    "        # is less than the inlier threshold, increment the number of inliers\n",
    "        dif = np.linalg.norm(exp_coord[0:2] - point2 )\n",
    "        if inlier_thresh > dif:\n",
    "            num_inliers = num_inliers+1\n",
    "\n",
    "      # If for this transformation we have found the most inliers update most_inliers and xform\n",
    "    if num_inliers > most_inliers:\n",
    "        most_inliers = num_inliers\n",
    "        xform = np.copy(np.float32(transformation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdBNvr_0J4pL"
   },
   "source": [
    "**Then, after we have our optimal transform xform, warp our img1 with that transformation, and display the result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAc-Bw76YSCa"
   },
   "outputs": [],
   "source": [
    "print(xform)\n",
    "output = cv2.warpPerspective(img1, xform, (cols, rows))\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Week 5) Image Alignment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
